{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Handwritten digit recognition using Single Layer NN\n",
    "\n",
    "The input dataset `digits` consists of 8x8 pixel images of handwritten digits. The goal is to recognize a handwritten digit. \n",
    "\n",
    "* Since an input image has 8 x 8 pixels, we have totally 8x8 = 64 features in the dataset. \n",
    "* Also since there are 10 digits, the output layer (the only layer) consists of 10 neurons.\n",
    "\n",
    "Therefore the weight matrix has the shape (64, 10).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "Y = digits.target\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we split the dataset randomly into training dataset and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `MLPClassifier` \n",
    "\n",
    "`MLPClassifier` is a multilayer NN based classifier available in the `neural_network` module. This classifier is designed to solve multi-class classification problem.\n",
    "\n",
    "Next we initialize the `MlPClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 18.22289732\n",
      "Iteration 2, loss = 18.90258766\n",
      "Iteration 3, loss = 7.21698742\n",
      "Iteration 4, loss = 3.60271548\n",
      "Iteration 5, loss = 2.18727180\n",
      "Iteration 6, loss = 1.66369590\n",
      "Iteration 7, loss = 1.22367224\n",
      "Iteration 8, loss = 1.03924409\n",
      "Iteration 9, loss = 1.05751346\n",
      "Iteration 10, loss = 0.85721144\n",
      "Iteration 11, loss = 0.73536822\n",
      "Iteration 12, loss = 0.70962511\n",
      "Iteration 13, loss = 0.63635816\n",
      "Iteration 14, loss = 0.56852174\n",
      "Iteration 15, loss = 0.49926645\n",
      "Iteration 16, loss = 0.51095472\n",
      "Iteration 17, loss = 0.46417317\n",
      "Iteration 18, loss = 0.42132958\n",
      "Iteration 19, loss = 0.41869454\n",
      "Iteration 20, loss = 0.31748498\n",
      "Iteration 21, loss = 3.46606933\n",
      "Iteration 22, loss = 1.34250669\n",
      "Iteration 23, loss = 0.99113015\n",
      "Iteration 24, loss = 0.68866130\n",
      "Iteration 25, loss = 0.59392578\n",
      "Iteration 26, loss = 0.46921842\n",
      "Iteration 27, loss = 0.43380799\n",
      "Iteration 28, loss = 0.32825090\n",
      "Iteration 29, loss = 0.31107282\n",
      "Iteration 30, loss = 0.22561221\n",
      "Iteration 31, loss = 0.18999015\n",
      "Iteration 32, loss = 0.14816151\n",
      "Iteration 33, loss = 0.15151377\n",
      "Iteration 34, loss = 0.13750028\n",
      "Iteration 35, loss = 0.13659896\n",
      "Iteration 36, loss = 0.11868513\n",
      "Iteration 37, loss = 0.07577118\n",
      "Iteration 38, loss = 0.08044231\n",
      "Iteration 39, loss = 0.07582166\n",
      "Iteration 40, loss = 0.04882516\n",
      "Iteration 41, loss = 0.04804162\n",
      "Iteration 42, loss = 0.02834207\n",
      "Iteration 43, loss = 0.01991520\n",
      "Iteration 44, loss = 0.02352882\n",
      "Iteration 45, loss = 0.00601203\n",
      "Iteration 46, loss = 0.00601194\n",
      "Iteration 47, loss = 0.36245766\n",
      "Iteration 48, loss = 0.10292104\n",
      "Iteration 49, loss = 0.07806835\n",
      "Iteration 50, loss = 0.04978997\n",
      "Iteration 51, loss = 0.02243361\n",
      "Iteration 52, loss = 0.01278949\n",
      "Iteration 53, loss = 0.00475074\n",
      "Iteration 54, loss = 0.00362324\n",
      "Iteration 55, loss = 0.00072238\n",
      "Iteration 56, loss = 0.00577600\n",
      "Iteration 57, loss = 0.00498461\n",
      "Iteration 58, loss = 0.00220185\n",
      "Iteration 59, loss = 0.00013533\n",
      "Iteration 60, loss = 0.00021637\n",
      "Iteration 61, loss = 0.00001585\n",
      "Iteration 62, loss = 0.00000666\n",
      "Iteration 63, loss = 0.00000701\n",
      "Iteration 64, loss = 0.00000705\n",
      "Iteration 65, loss = 0.00000689\n",
      "Iteration 66, loss = 0.00000674\n",
      "Iteration 67, loss = 0.00000650\n",
      "Iteration 68, loss = 0.00000615\n",
      "Iteration 69, loss = 0.00000596\n",
      "Iteration 70, loss = 0.00000571\n",
      "Iteration 71, loss = 0.00000554\n",
      "Iteration 72, loss = 0.00000537\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=0, hidden_layer_sizes=(), learning_rate_init=0.1,\n",
       "              solver='sgd', verbose=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(),  # No hidden Layer\n",
    "    alpha=0,\n",
    "    solver=\"sgd\",\n",
    "    verbose=True,\n",
    "    learning_rate_init=0.1,\n",
    ")\n",
    "mlp.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9495\n",
      "Confusion matrix:\n",
      "[[57  0  0  0  0  1  0  0  0  0]\n",
      " [ 0 49  0  1  2  0  0  0  1  1]\n",
      " [ 0  0 57  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 47  0  0  0  0  0  0]\n",
      " [ 0  4  0  0 68  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 62  0  0  0  0]\n",
      " [ 0  0  0  0  2  0 52  0  1  0]\n",
      " [ 0  0  0  1  0  0  0 55  1  0]\n",
      " [ 0  0  0  5  0  3  0  0 51  3]\n",
      " [ 0  0  0  1  0  1  0  0  2 66]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "Y_pred = mlp.predict(X_test)\n",
    "print('Accuracy = %0.4f'%np.mean(Y_pred == Y_test))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class probabilities can be predicted using the `predict_proba` method as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.12657890e-171, 3.81643305e-070, 1.01683703e-171,\n",
       "        6.55023655e-212, 9.92632230e-077, 2.88341969e-124,\n",
       "        1.07501095e-196, 1.00495687e-132, 1.00000000e+000,\n",
       "        3.16991597e-092]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.predict_proba(X_test[np.newaxis, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.predict(X_test[np.newaxis, 0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
